{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2a8ff4-93ef-4483-81c3-ef49fa616180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#segmentation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#Tokenize text to sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#Tokenize sentence in the text to words\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Remove the stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#Perfomr Steming and lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet') # download for lemmatization\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f271d112-fd41-47f5-add0-5c4410a95188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model and add neuralcoref to the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def standardize_text(text, print_parsed_text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    # Segmentation\n",
    "    sentences = sent_tokenize(text)\n",
    "    lemmatized_all = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        new_text = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentence)  # Remove extra spaces\n",
    "        new_text = re.sub(r\"\\s+\", \" \", new_text)  # Remove extra spaces\n",
    "        new_text = re.sub(r\"x+\\s\",\"\", new_text) # Remove X characters\n",
    "        \n",
    "        # Tokenization\n",
    "        words = word_tokenize(new_text)\n",
    "        \n",
    "        # StopWords\n",
    "        words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "        lemmatized_all.extend(lemmatized)\n",
    "        \n",
    "    unique_words = list(set(lemmatized_all))\n",
    "    processed_text = ' '.join(unique_words)\n",
    "    \n",
    "    # Named Entity Recognition (NER)\n",
    "    doc = nlp(processed_text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Dependency Parsing\n",
    "    dependencies = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "    if(print) :\n",
    "        # Print results\n",
    "        print(\"##################################################\")\n",
    "        print(f\"Original Text: {text}\")\n",
    "        print(\"##################################################\")\n",
    "        print(f\"Lemmatized Unique Words: {unique_words}\")\n",
    "        print(\"##################################################\")\n",
    "        print(f\"Named Entities: {entities}\")\n",
    "        print(\"##################################################\")\n",
    "        print(f\"Dependencies: {dependencies}\")\n",
    "    \n",
    "    return processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ef7e33-ed6e-4469-a051-8e455fd79aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Original Text: my xxxx  instructed me to write a letter because i am a xxxx  of xxxx xxxx.\n",
      "i am in a desperate need to fix this problem since i am trying to get things in order the accounts that i put below are due to the xxxx xxxx  violation that has nearly ruined my life please remove these accounts so i can start taking steps to getting my life back together. \n",
      "thank you so much for assisting me and offering these resources to xxxx  like me. \n",
      "account that are fraud due to xxxx xxxx  : xxxx. xxxx xxxxxxxx xxxx xxxx  opened xx/xx/xxxx xxxx. xxxx xxxxxxxx xxxx xxxx opened xx/xx/xxxx xxxx. xxxx  xxxx xxxx opened xx/xx/xxxx xxxx. the xxxx inquiries below | the only inquiry i did was the xxxx  xxxx xxxx in xxxx, xxxx\n",
      "##################################################\n",
      "Lemmatized Unique Words: ['step', 'offering', 'put', 'desperate', 'thing', 'trying', 'together', 'write', 'since', 'account', 'fithis', 'instructed', 'letter', 'need', 'please', 'back', 'like', 'inquiry', 'much', 'violation', 'xxxx', 'nearly', 'start', 'opened', 'order', 'life', 'resource', 'get', 'problem', 'taking', 'due', 'remove', 'getting', 'assisting', 'thank', 'fraud', 'ruined']\n",
      "##################################################\n",
      "Named Entities: []\n",
      "##################################################\n",
      "Dependencies: [('step', 'compound', 'offering'), ('offering', 'nsubj', 'put'), ('put', 'ROOT', 'put'), ('desperate', 'amod', 'thing'), ('thing', 'dobj', 'put'), ('trying', 'advcl', 'put'), ('together', 'advmod', 'write'), ('write', 'xcomp', 'trying'), ('since', 'mark', 'need'), ('account', 'nmod', 'letter'), ('fithis', 'det', 'letter'), ('instructed', 'amod', 'letter'), ('letter', 'nsubj', 'need'), ('need', 'advcl', 'put'), ('please', 'intj', 'need'), ('back', 'advmod', 'need'), ('like', 'prep', 'back'), ('inquiry', 'pobj', 'like'), ('much', 'amod', 'violation'), ('violation', 'nsubj', 'get'), ('xxxx', 'nsubj', 'start'), ('nearly', 'advmod', 'start'), ('start', 'relcl', 'violation'), ('opened', 'amod', 'resource'), ('order', 'compound', 'resource'), ('life', 'compound', 'resource'), ('resource', 'dobj', 'start'), ('get', 'ccomp', 'need'), ('problem', 'dobj', 'get'), ('taking', 'acl', 'problem'), ('due', 'advmod', 'taking'), ('remove', 'ccomp', 'get'), ('getting', 'xcomp', 'remove'), ('assisting', 'xcomp', 'getting'), ('thank', 'compound', 'fraud'), ('fraud', 'dobj', 'assisting'), ('ruined', 'dep', 'put')]\n",
      "##################################################\n",
      "Processed Text: step offering put desperate thing trying together write since account fithis instructed letter need please back like inquiry much violation xxxx nearly start opened order life resource get problem taking due remove getting assisting thank fraud ruined\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"My XXXX  instructed me to write a letter because I am a XXXX  of XXXX XXXX.\n",
    "I am in a desperate need to fix this problem since I am trying to get things in order The Accounts that I put below are due to the XXXX XXXX  violation that has nearly ruined my life Please remove these accounts so I can start taking steps to getting my life back together. \n",
    "Thank you so much for assisting me and offering these resources to XXXX  like me. \n",
    "ACCOUNT THAT ARE FRAUD DUE TO XXXX XXXX  : XXXX. XXXX XXXXXXXX XXXX XXXX  opened XX/XX/XXXX XXXX. XXXX XXXXXXXX XXXX XXXX opened XX/XX/XXXX XXXX. XXXX  XXXX XXXX opened XX/XX/XXXX XXXX. The XXXX Inquiries Below | The only inquiry I did was the XXXX  XXXX XXXX in XXXX, XXXX\"\"\"\n",
    "\n",
    "# Example usage\n",
    "processed_text = standardize_text(text, True)\n",
    "print(\"##################################################\")\n",
    "print(f\"Processed Text: {processed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b25d3381-0a18-4b93-ada8-e8c20e523fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22989 entries, 0 to 22988\n",
      "Data columns (total 2 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   Product                       22989 non-null  object\n",
      " 1   Consumer complaint narrative  22989 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 359.3+ KB\n"
     ]
    }
   ],
   "source": [
    "file_loc = \"../Resources/ModelData/customer_complaint_data_to_process.csv\"\n",
    "customer_complaints_df = pd.read_csv(file_loc)\n",
    "customer_complaints_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828fe1c2-bffc-48da-8ad7-009aaff9881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>fault contacted someone reversed fraud receive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>saving reversed would give allowing marcus onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>chase loan give said called order put payment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product  \\\n",
       "0                  Credit card   \n",
       "1  Checking or savings account   \n",
       "2  Checking or savings account   \n",
       "\n",
       "                        Consumer complaint narrative  \n",
       "0  fault contacted someone reversed fraud receive...  \n",
       "1  saving reversed would give allowing marcus onl...  \n",
       "2  chase loan give said called order put payment ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_complaints_df.head(3)\n",
    "customer_complaints_df['Consumer complaint narrative'] = customer_complaints_df['Consumer complaint narrative'].apply(standardize_text)\n",
    "customer_complaints_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e4fa7f-bc75-4a1b-bf25-4b4f65c24372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>fault contacted someone reversed fraud receive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>saving reversed would give allowing marcus onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>chase loan give said called order put payment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product  \\\n",
       "0                  Credit card   \n",
       "1  Checking or savings account   \n",
       "2  Checking or savings account   \n",
       "\n",
       "                        Consumer complaint narrative  \n",
       "0  fault contacted someone reversed fraud receive...  \n",
       "1  saving reversed would give allowing marcus onl...  \n",
       "2  chase loan give said called order put payment ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_complaints_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f99005-8d2a-4f2e-baf2-52906144176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_complaints_df.to_csv(\"../Resources/ModelData/train_test_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ec274-ef0d-4c01-9341-76379ba09698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
