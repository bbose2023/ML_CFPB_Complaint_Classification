{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b558274-77e2-49b4-aec7-7e1f350e4391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"recently i checked my credit report and noticed multiple billing errors under 15 u.s. code 1666 ( b ) billing error ( 4 ) the creditors failure to reflect properly on a statement a payment made by the obligor or a credit issued to the obligor.( 5 ) a computation error or similar error of the accounting nature of the creditor on a statement. as i know the creditor didn't notify me 21 days before with a statement so there should be no late payments on these accounts. i demand to see hard solid proof from the creditor bank account that i didn't pay and was marked late because this clearly a billing error. the items below should all be updated to paid as agreed on my consumer report.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Recently I checked my credit report and noticed multiple billing errors under 15 U.S. Code 1666 ( b ) billing error ( 4 ) The creditors failure to reflect properly on a statement a payment made by the obligor or a credit issued to the obligor.( 5 ) A computation error or similar error of the accounting nature of the creditor on a statement. As I know the creditor didn't notify me 21 days before with a statement so there should be no late payments on these accounts. I demand to see hard solid proof from the creditor bank account that I didn't pay and was marked late because this clearly a billing error. The items below should all be updated to paid as agreed on my consumer report.\"\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2606b569-7b63-48d2-9efe-4f26b07db2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#segmentation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97b6f0db-5cc3-4e7a-9a8c-54cbe608c566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recently i checked my credit report and noticed multiple billing errors under 15 u.s. code 1666 ( b ) billing error ( 4 ) the creditors failure to reflect properly on a statement a payment made by the obligor or a credit issued to the obligor.',\n",
       " '( 5 ) a computation error or similar error of the accounting nature of the creditor on a statement.',\n",
       " \"as i know the creditor didn't notify me 21 days before with a statement so there should be no late payments on these accounts.\",\n",
       " \"i demand to see hard solid proof from the creditor bank account that i didn't pay and was marked late because this clearly a billing error.\",\n",
       " 'the items below should all be updated to paid as agreed on my consumer report.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "091fe876-5413-43e9-bfe1-59bf741054c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recently i checked my credit report and noticed multiple billing errors under 15 u s  code 1666   b   billing error   4   the creditors failure to reflect properly on a statement a payment made by the obligor or a credit issued to the obligor '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation removal\n",
    "import re\n",
    "\n",
    "# Remove punctuation characters\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentences[0]) \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c7055a4-b2e9-4676-8ce9-260cd63b93ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7347c410-c586-42dd-8860-2398e23a74e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recently',\n",
       " 'i',\n",
       " 'checked',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'report',\n",
       " 'and',\n",
       " 'noticed',\n",
       " 'multiple',\n",
       " 'billing',\n",
       " 'errors',\n",
       " 'under',\n",
       " '15',\n",
       " 'u',\n",
       " 's',\n",
       " 'code',\n",
       " '1666',\n",
       " 'b',\n",
       " 'billing',\n",
       " 'error',\n",
       " '4',\n",
       " 'the',\n",
       " 'creditors',\n",
       " 'failure',\n",
       " 'to',\n",
       " 'reflect',\n",
       " 'properly',\n",
       " 'on',\n",
       " 'a',\n",
       " 'statement',\n",
       " 'a',\n",
       " 'payment',\n",
       " 'made',\n",
       " 'by',\n",
       " 'the',\n",
       " 'obligor',\n",
       " 'or',\n",
       " 'a',\n",
       " 'credit',\n",
       " 'issued',\n",
       " 'to',\n",
       " 'the',\n",
       " 'obligor']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b67f4df-08b7-43df-958b-500103003c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c5531fe-0900-434a-a760-a91932821a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recently', 'checked', 'credit', 'report', 'noticed', 'multiple', 'billing', 'errors', '15', 'u', 'code', '1666', 'b', 'billing', 'error', '4', 'creditors', 'failure', 'reflect', 'properly', 'statement', 'payment', 'made', 'obligor', 'credit', 'issued', 'obligor']\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d603075f-f915-471b-b1fc-2123432e3be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f98be261-fb8f-4c3f-ae30-c85bade185fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') # download for lemmatization\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b6fc9ef-7eee-4059-b026-4a40285ced90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recent', 'check', 'credit', 'report', 'notic', 'multipl', 'bill', 'error', '15', 'u', 'code', '1666', 'b', 'bill', 'error', '4', 'creditor', 'failur', 'reflect', 'properli', 'statement', 'payment', 'made', 'obligor', 'credit', 'issu', 'obligor']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmed = [PorterStemmer().stem(w) for w in words ]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45029422-b31b-439c-8c47-13888d36ae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recently', 'checked', 'credit', 'report', 'noticed', 'multiple', 'billing', 'error', '15', 'u', 'code', '1666', 'b', 'billing', 'error', '4', 'creditor', 'failure', 'reflect', 'properly', 'statement', 'payment', 'made', 'obligor', 'credit', 'issued', 'obligor']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fb94a5d-f53d-43c5-a0dc-5114d60fbfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming output: ['wait', 'wait', 'studi', 'studi', 'comput', 'bill']\n",
      "Lemmatization output: ['wait', 'waiting', 'study', 'studying', 'computer', 'billing']\n"
     ]
    }
   ],
   "source": [
    "# Another stemming and lemmatization example\n",
    "words2 = ['wait', 'waiting' , 'studies', 'studying', 'computers','billing']\n",
    "\n",
    "# Stemming\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words2]\n",
    "print(\"Stemming output: {}\".format(stemmed))\n",
    "\n",
    "# Lemmatization\n",
    "# Reduce words to their root form\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words2]\n",
    "print(\"Lemmatization output: {}\".format(lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0da69a1b-bb86-49dd-8c9c-c93956ec8a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f39c66-0828-4a09-92f4-a3a0b8b05b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\boseb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#Tokenize text to sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#Tokenize sentence in the text to words\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Remove the stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#Perfomr Steming and lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet') # download for lemmatization\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7994502e-20a3-49e1-9230-4acec28ab526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert CSV files to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad30c7a-aea6-438d-96e2-231187304894",
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": []
=======
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to split the CSV into chunks and save each chunk\n",
    "def split_and_save_csv(file_path, chunk_size, output_prefix):\n",
    "    # Read the CSV file in chunks\n",
    "    reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    \n",
    "    for i, chunk in enumerate(reader):\n",
    "        output_file = f\"{output_prefix}_chunk{i}.csv\"\n",
    "        chunk.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {output_file}\")\n",
    "\n",
    "# Function to run the above function in multiple threads\n",
    "def multi_threaded_split(file_path, chunk_size, output_prefix, num_threads):\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Use list of futures to split the work among threads\n",
    "        futures = []\n",
    "        reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "        for i, chunk in enumerate(reader):\n",
    "            output_file = f\"{output_prefix}_chunk{i}.csv\"\n",
    "            futures.append(executor.submit(chunk.to_csv, output_file, index=False))\n",
    "        \n",
    "        # Ensure all futures are completed\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "        print(\"All chunks have been processed and saved.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'large_file.csv'  # Path to your large CSV file\n",
    "chunk_size = 10000  # Number of rows per chunk\n",
    "output_prefix = 'output'  # Prefix for the output files\n",
    "num_threads = 4  # Number of threads to use\n",
    "\n",
    "multi_threaded_split(file_path, chunk_size, output_prefix, num_threads)\n"
   ]
>>>>>>> origin
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3baf6c4-8977-46d7-b170-c9910a6045a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
