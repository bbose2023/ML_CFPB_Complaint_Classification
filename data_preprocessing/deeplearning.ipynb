{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13fe49de-09f4-4840-8797-ceadc5961c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6700bf5-1807-4392-85c5-358565b670c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anirudhb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anirudhb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anirudhb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f433db-7932-44e4-9370-5ccca74608f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>16 date description debit 60 2 purchase 30 9 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>due longer since notified dollar received leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>fcra asked due longer since please ive 19 plen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>unwanted letter due additional alert received ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>investigation also prehistoric issue possibly ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product  \\\n",
       "0                  Credit card   \n",
       "1  Checking or savings account   \n",
       "2                  Credit card   \n",
       "3                  Credit card   \n",
       "4                  Credit card   \n",
       "\n",
       "                        Consumer complaint narrative  \n",
       "0  16 date description debit 60 2 purchase 30 9 6...  \n",
       "1  due longer since notified dollar received leav...  \n",
       "2  fcra asked due longer since please ive 19 plen...  \n",
       "3  unwanted letter due additional alert received ...  \n",
       "4  investigation also prehistoric issue possibly ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Import and read the complaint_data.csv.\n",
    "complaint_Data_df = pd.read_csv(\"../Resources/ModelData/train_test_data (1).csv\")\n",
    "complaint_Data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85363a43-d5f1-48f2-8f7c-3eb6c843eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dffe0eaf-1799-411f-b874-6340024687f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complaint_Data_df['Consumer complaint narrative']  # Features\n",
    "y = complaint_Data_df['Product']  # Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2accdbc-9bbf-4f22-ab68-651e4f69419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is our feature set and y is our target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, #20% of the data is allocated to the test set, and 80% to the training set.\n",
    "    stratify=y, #Ensures the distribution of y categories is preserved in the split.\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06441231-28b7-4201-8ddb-3081fac688a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train sequence: [201, 75, 169, 703, 73, 667, 544, 63, 369, 3, 603, 355, 24, 687, 330, 559, 609, 688, 160, 204, 242, 247, 645, 121, 668, 200, 488, 93, 433]\n",
      "Sample test sequence: [54, 27, 860, 67, 98, 16, 349, 4610, 102, 236, 24, 97, 365, 792, 616, 57, 280, 454, 700, 3784, 363, 623, 972, 301, 288, 999, 103, 449, 1111, 130, 1047, 410, 109, 95, 237, 185, 1095, 322, 1245, 129, 269, 110, 1, 14, 281, 26, 551, 3814, 872, 46, 108, 300, 239, 25, 971, 299, 49, 184, 401, 162, 52, 31, 80, 470, 196, 60, 910, 387, 59, 659, 666, 174, 1642, 78, 4, 219, 583, 6, 362, 1513, 121, 3040, 158, 29, 43, 529, 1287, 1182, 233, 71, 35, 920, 1104, 96, 354, 28, 18, 5707, 169, 2940, 1009, 471, 3, 38, 159, 3672, 11, 84, 1067, 76, 48, 1026, 128, 89, 7, 418, 51, 419, 20, 53, 5, 132, 545]\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the text\n",
    "# Define tokenizer with a vocabulary size of 25,000\n",
    "tokenizer = Tokenizer(num_words=25000)\n",
    "\n",
    "# Fit tokenizer on training data\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('../Resources/ModelData/tokenizer_3month_2product.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# Transform training and testing text data into sequences of integers\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train.values)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.values)\n",
    "\n",
    "# Print sample outputs for verification\n",
    "print(\"Sample train sequence:\", train_sequences[0])\n",
    "print(\"Sample test sequence:\", test_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81cca21-b9bf-4987-bf5b-cd930c345e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14092 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Extract the word index (mapping of words to their respective indices)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f79d7a6d-52c0-4161-8b6c-3996d19979af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14093"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_index)+1 #where +1 accounts for an additional index for special tokens like padding or unknown tokens.\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ecca43-5d4a-4f75-9842-bedf97cea58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 718\n",
      "Average length: 69.32749840865691\n",
      "Median length: 56.0\n",
      "90th percentile: 134.0\n",
      "95th percentile: 168.0\n"
     ]
    }
   ],
   "source": [
    "# Get lengths of all sequences in the dataset\n",
    "sequence_lengths = [len(text.split()) for text in X_train.values]\n",
    "# Compute statistics\n",
    "import numpy as np\n",
    "print(\"Max length:\", np.max(sequence_lengths))\n",
    "print(\"Average length:\", np.mean(sequence_lengths))\n",
    "print(\"Median length:\", np.median(sequence_lengths))\n",
    "print(\"90th percentile:\", np.percentile(sequence_lengths, 90))\n",
    "print(\"95th percentile:\", np.percentile(sequence_lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f698a59-7c5b-41ca-9513-91ec70414494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_data shape: (6284, 718)\n",
      "X_test_data shape: (1572, 718)\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences to ensure equal length(This approach reduces unnecessary padding while retaining most of the relevant data)\n",
    "X_train_data = pad_sequences(train_sequences, maxlen=718, padding=\"post\", truncating='post')#(Example maxlen based on our data analysis)\n",
    "X_test_data = pad_sequences(test_sequences, maxlen=718, padding=\"post\", truncating='post')#Truncates sequences longer than maxlen from the end.\n",
    "print(f\"X_train_data shape: {X_train_data.shape}\")\n",
    "print(f\"X_test_data shape: {X_test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2dac83-c544-4cf1-92cd-f9db3219e27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mapping: {0: 'Checking or savings account', 1: 'Credit card'}\n",
      "Encoded y_train sample: [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training and test target variables\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Check the mapping of labels\n",
    "print(\"Class Mapping:\", dict(enumerate(label_encoder.classes_)))\n",
    "print(\"Encoded y_train sample:\", y_train_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c176d9a-454c-478c-a5d9-0fd328e52b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (6284, 718)\n",
      "Shape of label tensor: (6284, 2)\n",
      "Shape of label tensor: (1572, 2)\n"
     ]
    }
   ],
   "source": [
    "# Apply one-hot encoding to labels\n",
    "labels_train = to_categorical(np.asarray(y_train_encoded))\n",
    "labels_test = to_categorical(np.asarray(y_test_encoded))\n",
    "# Print shapes again to verify\n",
    "print('Shape of data tensor:', X_train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20ed29d-6d1d-4131-b47c-c2aa8c50a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bac4e6d-0831-45e5-a3c2-7688f30e115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model.add(Embedding(vocabulary_size,                # Vocabulary size\n",
    "                    output_dim=300,                # Embedding size (vector size)\n",
    "                    input_length=718,              # Maximum length of input sequences\n",
    "                    trainable=True))               # Embeddings are trainable\n",
    "\n",
    "# Add LSTM layer\n",
    "#Bidirectional LSTM\n",
    "model.add(Bidirectional(LSTM(300, dropout = 0.2, recurrent_dropout = 0.2)))\n",
    "\n",
    "# Add Dense output layer for multiclass classification\n",
    "model.add(Dense(2, activation='softmax'))  # 2 classes for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70d09891-fc5e-4e89-99a7-6d0368b883c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529f8ffe-5827-46b5-9a32-9255cb8be438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 2s/step - accuracy: 0.6874 - loss: 0.5819 - val_accuracy: 0.8791 - val_loss: 0.2915\n",
      "Epoch 2/3\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 2s/step - accuracy: 0.9270 - loss: 0.2072 - val_accuracy: 0.9122 - val_loss: 0.2482\n",
      "Epoch 3/3\n",
      "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 2s/step - accuracy: 0.9571 - loss: 0.1294 - val_accuracy: 0.8931 - val_loss: 0.2757\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_data, labels_train,\n",
    " batch_size=64,\n",
    " epochs=3,\n",
    " validation_data=(X_test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "206a145b-207b-4cd8-a137-076b067f8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM and tokenizer saved with pickle.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "with open('../Resources/ModelData/LSTM_model_3month_2product.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "# Save the vectorizer\n",
    "with open('../Resources/ModelData/tokenizer_3month_2product.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(tokenizer, vectorizer_file)\n",
    "print(\"LSTM and tokenizer saved with pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f49f610-959b-4014-93eb-d7d127adca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "with open('../Resources/ModelData/LSTM_model_3month_2product.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "# Load the vectorizer\n",
    "with open('../Resources/ModelData/tokenizer_3month_2product.pkl', 'rb') as vectorizer_file:\n",
    "    loaded_vectorizer = pickle.load(vectorizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8c89532-1e4c-4446-b2b2-d2bc2baae7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 721ms/step\n",
      "[[0.03463621 0.96536386]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03383401 0.966166  ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03327135 0.9667287 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457818 0.96542186]\n",
      " [0.034197   0.9658031 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03460999 0.9653901 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.0337301  0.96626985]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457818 0.96542186]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03387108 0.966129  ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.03457818 0.96542186]\n",
      " [0.03457818 0.96542186]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.035292   0.96470803]\n",
      " [0.03364487 0.9663552 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.0342478  0.96575224]\n",
      " [0.03463621 0.96536386]\n",
      " [0.035292   0.96470803]\n",
      " [0.03364487 0.9663552 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03396448 0.96603554]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03327135 0.9667287 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03411173 0.9658882 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03387108 0.966129  ]\n",
      " [0.034197   0.9658031 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.0337301  0.96626985]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457818 0.96542186]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.0337301  0.96626985]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03414589 0.96585405]\n",
      " [0.03383401 0.966166  ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03396448 0.96603554]\n",
      " [0.03457818 0.96542186]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03387108 0.966129  ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03460999 0.9653901 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03411173 0.9658882 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03396448 0.96603554]\n",
      " [0.03457818 0.96542186]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03414589 0.96585405]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.034197   0.9658031 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03387108 0.966129  ]\n",
      " [0.034197   0.9658031 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03396448 0.96603554]\n",
      " [0.03463621 0.96536386]\n",
      " [0.03457818 0.96542186]\n",
      " [0.03457818 0.96542186]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.03387108 0.966129  ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.0337301  0.96626985]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03396448 0.96603554]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.0342478  0.96575224]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03376018 0.9662398 ]\n",
      " [0.03387608 0.966124  ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457818 0.96542186]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03383401 0.966166  ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.035292   0.96470803]\n",
      " [0.03458719 0.96541286]\n",
      " [0.03411173 0.9658882 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.03327135 0.9667287 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.03411173 0.9658882 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.0342478  0.96575224]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03383401 0.966166  ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03383401 0.966166  ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03463621 0.96536386]\n",
      " [0.03387108 0.966129  ]\n",
      " [0.03396448 0.96603554]\n",
      " [0.03411173 0.9658882 ]\n",
      " [0.03383401 0.966166  ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03387108 0.966129  ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03396448 0.96603554]\n",
      " [0.03411173 0.9658882 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03460999 0.9653901 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.035292   0.96470803]\n",
      " [0.03463621 0.96536386]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.0337301  0.96626985]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03457643 0.9654236 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.03458719 0.96541286]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03383401 0.966166  ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03516963 0.9648304 ]\n",
      " [0.03459322 0.9654068 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03387608 0.966124  ]\n",
      " [0.03387608 0.966124  ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03345055 0.9665494 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03327135 0.9667287 ]\n",
      " [0.0337301  0.96626985]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.02829746 0.9717025 ]\n",
      " [0.03516965 0.9648304 ]\n",
      " [0.03459324 0.9654068 ]\n",
      " [0.03387609 0.966124  ]\n",
      " [0.02829747 0.9717025 ]\n",
      " [0.03387608 0.96612394]\n",
      " [0.03387608 0.96612394]\n",
      " [0.02829746 0.9717026 ]]\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "texts = '''I recently XXXX  a XXXX XXXX and I was in the XXXX  and also I am still seeing a XXXX XXXX  and taking tests. Because of my health problems I was unable to pay my bills on time. The bank added 10 late fees for insufficient funds. I asked for reimbursement but was denied. The fees are XXXX XXXX {$35.00} each= {$350.00}'''\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences if necessary (depends on your model input requirements)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=718)\n",
    "\n",
    "\n",
    "# Predict using the model\n",
    "predictions = loaded_model.predict(padded_sequences)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a9c1c1d-b381-4d9e-bca6-e6f2d42d2df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support \u001b[38;5;28;01mas\u001b[39;00m score\n\u001b[0;32m----> 4\u001b[0m precision, recall, fscore, support \u001b[38;5;241m=\u001b[39m \u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(precision))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(recall))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythondata/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythondata/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1767\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \n\u001b[1;32m   1606\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1767\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythondata/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1539\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1539\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythondata/lib/python3.10/site-packages/sklearn/metrics/_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     96\u001b[0m             type_true, type_pred\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(predictions, predictions.round())\n",
    "print('precision: \\n{}'.format(precision))\n",
    "print('recall: \\n{}'.format(recall))\n",
    "print('fscore: \\n{}'.format(fscore))\n",
    "print('support: \\n{}'.format(support))\n",
    "Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e913f-7dbc-46dd-b88d-22054fbf143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_test, predicted.round(),target_names=df1['product'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69b75fa8-4819-47e1-b5f0-9e35bbf5cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[[2676], [2661], [], [744], [2465], [602], [7967], [], [], [1796], [3350], [602], [], [], [2111], [602], [602], [], [175], [551], [], [], [], [3350], [12895], [1637], [1719], [], [551], [551], [], [], [602], [1637], [12895], [602], [], [], [744], [12895], [2465], [2465], [], [2111], [73], [1637], [], [], [4895], [602], [], [2465], [], [4895], [], [], [602], [1796], [], [], [138], [258], [551], [], [], [2465], [1796], [], [], [138], [258], [], [2465], [], [], [602], [], [2661], [602], [12895], [7967], [744], [73], [2661], [1796], [602], [7967], [602], [1637], [], [], [744], [], [1637], [3350], [], [], [], [], [602], [], [], [73], [1637], [], [744], [2465], [602], [], [12895], [1637], [1796], [73], [2111], [2111], [12895], [330], [12895], [602], [1637], [], [], [], [12895], [7967], [602], [], [1796], [], [12895], [2465], [2465], [], [1796], [602], [602], [12895], [1637], [1719], [], [2661], [602], [330], [602], [1637], [], [2465], [11523], [], [2676], [], [11523]]\n",
      "718\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(sequence)) \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#print(len(y_test))\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m rounded_labels\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#rounded_labels[1]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Example test data (replace with your actual test data)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythondata/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pythondata/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# Assuming `model` is your trained model\n",
    "model.save('LSTM_model_3month_2Products.h5')\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the tokenizer \n",
    "with open('../Resources/ModelData/tokenizer_3month_2product.pkl', 'rb') as handle: \n",
    "\ttokenizer = pickle.load(handle)\n",
    "\t\n",
    "# Load the model\n",
    "model = load_model('LSTM_model_3month_2Products.h5')\n",
    "\n",
    "# Single test example\n",
    "text = '''problem asked fee 10 taking 00 denied bill fund health test 350 also 35 late reimbursement bank added unable insufficient time still seeing recently pay'''\n",
    "words = text.split(\" \")\n",
    "print(len(words))\n",
    "\n",
    "# Tokenize and pad the text\n",
    "sequence = tokenizer.texts_to_sequences(text)\n",
    "print(sequence)\n",
    "X_test = pad_sequences(sequence, maxlen=718)  # Adjust maxlen based on your model's requirements\n",
    "print(len(X_test[2]))\n",
    "# Expected output (label)\n",
    "y_test = np.array([1]*len(sequence)) \n",
    "#print(len(y_test))\n",
    "rounded_labels=np.argmax(y_test, axis=1)\n",
    "#rounded_labels[1]\n",
    "\n",
    "# Example test data (replace with your actual test data)\n",
    "import numpy as np\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Assuming you are working on a classification problem\n",
    "y_pred = (predictions > 0.5).astype(\"int32\")  # Adjust threshold for your specific case\n",
    "print(len(y_pred))\n",
    "print(len(rounded_labels))\n",
    "print(len(sequence))\n",
    "# Calculate metrics\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(rounded_labels, y_pred, average='binary')\n",
    "accuracy = accuracy_score(rounded_labels, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F-score: {fscore}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77406b-4632-46b2-a34f-3797d1c43ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
